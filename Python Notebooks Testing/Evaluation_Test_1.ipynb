{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca3b462-6af9-4dd0-98ef-067591b18140",
   "metadata": {},
   "source": [
    "### Evaluation & Metrics: Assess model performance using relevant metrics: perplexity, distinct, Self-BLEU\n",
    "\n",
    "#### Perplexity:\n",
    "measures how “surprised” a language model is by a given text. Lower perplexity means the model assigns higher probability to the text\n",
    ">Lower scores indicate the model is better at modeling the style/content of your corpus. Extremely low scores can also hint at overly safe or repetitive outputs.\n",
    ">\n",
    "\n",
    "#### Distinct:\n",
    "quantifies how many unique n-grams appear in the generated texts, relative to the total number of n-grams produced. It’s a simple diversity measure\n",
    ">\t•\tDistinct-1 measures word-level diversity (unigrams). Distinct-2 measures phrase-level diversity (bigrams). Values closer to 1.0 mean high diversity (few repeats); values closer to 0 mean the model is repeating the same words/phrases.\n",
    "\n",
    "####  Self-BLEU:\n",
    "evaluates how similar the generated samples are to one another. It’s a reverse of BLEU: treating each generation as a “hypothesis” and all the others as “references.”\n",
    ">\t•\tScores range from 0 to 1. Higher Self-BLEU means samples are very similar to each other (low diversity). Lower Self-BLEU means samples are more distinct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae0624b-6292-46b9-a30d-60aae7f1fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf63f2f1-313a-4582-bed0-d00450c94644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perplexity based on GPT2\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def calc_perplexity(text: str) -> float:\n",
    "    \"\"\"perplexity\"\"\"\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        neg_log_likelihood = outputs.loss * input_ids.shape[1]\n",
    "    ppl = torch.exp(neg_log_likelihood / input_ids.shape[1])\n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c508c7a-b9cb-4379-be8b-d53eb79524f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distinct\n",
    "\n",
    "def distinct_n(texts, n=1):\n",
    "    \"\"\"Distinct-n = (#unique n-grams) / (#total n-grams)\"\"\"\n",
    "    total_ngrams = 0\n",
    "    unique_ngrams = set()\n",
    "    for txt in texts:\n",
    "        tokens = txt.split()\n",
    "        ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "        count = 0\n",
    "        for ng in ngrams:\n",
    "            unique_ngrams.add(ng)\n",
    "            count += 1\n",
    "        total_ngrams += count\n",
    "    return len(unique_ngrams) / total_ngrams if total_ngrams > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b489e3d-16c9-4a7e-8d81-051e7064ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLEU\n",
    "\n",
    "def self_bleu(texts, n_gram=4):\n",
    "    \"\"\"\n",
    "   Self-BLEU\n",
    "    \"\"\"\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    scores = []\n",
    "    for i, cand in enumerate(texts):\n",
    "        references = [t.split() for j,t in enumerate(texts) if j != i]\n",
    "        scores.append(sentence_bleu(\n",
    "            references=[references],       \n",
    "            hypothesis=cand.split(),\n",
    "            smoothing_function=smoothie,\n",
    "            weights=tuple([1/n_gram]*n_gram)  \n",
    "        ))\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "177b2fcf-8a26-4212-9e0f-112798c24f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities: [46.377098083496094]\n",
      "Avg Perplexity: 46.38\n",
      "Distinct-1: 0.8889\n",
      "Distinct-2: 1.0000\n",
      "Self-BLEU (up to 4-gram): 0.0000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    generated_texts = [\"When first mine eyes beheld thy gentle face, and I saw the light of thylips, which was in the midst of my mind, as it were. I took a little while to look at the picture\" ] #input generated poem \n",
    "\n",
    "    # 4.1 Perplexity\n",
    "    ppls = [calc_perplexity(txt) for txt in generated_texts]\n",
    "    print(\"Perplexities:\", ppls)\n",
    "    print(f\"Avg Perplexity: {sum(ppls)/len(ppls):.2f}\")\n",
    "\n",
    "    # 4.2 Distinct-1 / Distinct-2\n",
    "    print(f\"Distinct-1: {distinct_n(generated_texts, n=1):.4f}\")\n",
    "    print(f\"Distinct-2: {distinct_n(generated_texts, n=2):.4f}\")\n",
    "\n",
    "    # 4.3 Self-BLEU\n",
    "    sb = self_bleu(generated_texts, n_gram=4)\n",
    "    print(f\"Self-BLEU (up to 4-gram): {sb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9dcc7b-cce2-416a-9e01-56129bf9c10f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
